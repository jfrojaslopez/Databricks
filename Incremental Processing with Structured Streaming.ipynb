{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7af853e2-b63b-44b8-8537-720bf6909557",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Incremental Processing with Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e2ec323-703f-4d17-8dae-8efcd9114b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Streaming Data Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed3f87f-1766-4d5a-a57f-af323e916559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"background-color: lightblue; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**¿Qué son los datos en flujo?**\n",
    "\n",
    "Datos ilimitados generados continuamente\n",
    "\n",
    "**Fuentes de datos típicas**\n",
    "\n",
    "1. Fuentes de datos de cambios en la base de datos\n",
    "2. Flujos de clics\n",
    "3. Registros de máquinas y aplicaciones\n",
    "4. Eventos de aplicación\n",
    "5. Datos móviles y de IoT\n",
    "\n",
    "***La gran mayoría de los datos del mundo son datos en streaming.***\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8d24863-7c20-4d22-b9f8-a48462e4a07a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Notas:**\n",
    "\n",
    "Los datos en flujo o streaming data son un tipo de datos generados de forma continua y sin límite, típicamente en pequeñas cantidades, que son procesados en tiempo real o casi en tiempo real. Estos datos provienen de diversas fuentes y suelen reflejar eventos que ocurren en sistemas, aplicaciones o dispositivos. Los datos en flujo son fundamentales en muchos escenarios donde la información debe ser procesada o monitoreada de manera inmediata.\n",
    "\n",
    "**Características clave de los datos en flujo:**\n",
    "\n",
    "- Generación continua: Los datos no se producen en bloques predefinidos, sino que son generados y transmitidos de forma constante.\n",
    "- Bajo volumen por evento: Cada evento individual generalmente contiene pocos datos, pero el flujo continuo de eventos puede resultar en un gran volumen con el tiempo.\n",
    "- Procesamiento en tiempo real: Los datos deben ser procesados de inmediato, sin esperar a que se almacenen en una base de datos tradicional.\n",
    "\n",
    "**Fuentes típicas de datos en flujo:**\n",
    "\n",
    "- **DB change data feeds:** Cambios en bases de datos que se envían en tiempo real, por ejemplo, modificaciones de registros.\n",
    "- **Clickstreams:** Datos generados por los clics de los usuarios en sitios web o aplicaciones, permitiendo el análisis de navegación en tiempo real.\n",
    "- **Logs de máquinas y aplicaciones:** Información continua generada por sistemas informáticos, aplicaciones o dispositivos de hardware que registran eventos de sistema.\n",
    "- **Eventos de aplicaciones:** Eventos generados por aplicaciones que requieren ser monitoreados o respondidos rápidamente, como alertas de sistemas.\n",
    "- **Datos de dispositivos móviles e IoT:** Sensores, dispositivos conectados y aplicaciones móviles que generan datos constantemente, que pueden abarcar desde lecturas de sensores hasta actualizaciones de posición GPS.\n",
    "\n",
    "**Importancia:**\n",
    "\n",
    "Los datos en flujo representan una gran parte del volumen de datos generado en el mundo moderno, ya que muchas aplicaciones actuales dependen de la capacidad de procesar información en tiempo real, como en sistemas financieros, control de dispositivos IoT, monitoreo de aplicaciones web y análisis de comportamiento de usuarios en línea.\n",
    "\n",
    "Estos datos requieren tecnologías y plataformas especializadas para su procesamiento, como **Apache Kafka, Amazon Kinesis, Azure Event Hubs**, y otras herramientas diseñadas para soportar la ingesta y procesamiento de eventos a gran escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f96f10-d352-4da5-a1bd-93fb6322cf3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; background-color: lightblue; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**¿Qué es el procesamiento de flujos?**\n",
    "\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "  El tratamiento tradicional de datos por lotes es puntual y limitado.\n",
    "\n",
    "  </div>\n",
    "\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "  El procesamiento en flujo es continuo e ilimitado\n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9687e4-08db-4b97-bc01-e0bb23798114",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"background-color: lightblue; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Procesamiento de Stream**\n",
    "\n",
    "¿Por qué se está popularizando el procesamiento de flujos?\n",
    "\n",
    "- **Velocidad y volumen de los datos:** El aumento de la velocidad y el volumen de los datos requiere un procesamiento continuo e incremental - no se pueden procesar todos los datos en un lote programado\n",
    "\n",
    "- **Análisis en tiempo real:** Las empresas exigen acceso a datos nuevos para obtener información práctica y tomar mejores decisiones empresariales con mayor rapidez.\n",
    "\n",
    "- **Aplicaciones operativas:** Las aplicaciones críticas necesitan datos en tiempo real para una respuesta eficaz e instantánea.\n",
    "\n",
    "***La gran mayoría de los datos del mundo son datos en flujo.***\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc5c0cc2-de03-4dad-b9b1-2374a76c095f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Casos prácticos de procesamiento de flujos\n",
    "\n",
    "El procesamiento de flujos es un componente clave de las aplicaciones de big data en todas las industrias\n",
    "\n",
    "1. Notificaciones\n",
    "2. Informes en tiempo real\n",
    "3. ETL incremental\n",
    "4. Actualización de datos para servir en tiempo real\n",
    "5. Toma de decisiones en tiempo real\n",
    "6. ML en línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a890f8-89de-4801-8c32-ba5c2187a28e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Notas:**\n",
    "\n",
    "Los casos prácticos de procesamiento de flujos son esenciales para aplicaciones de big data, ya que permiten el manejo de grandes volúmenes de datos que se generan en tiempo real. Aquí tienes una explicación más detallada de cada caso:\n",
    "\n",
    "1. **Notificaciones:**\n",
    "\n",
    "  - **Ejemplo:** Plataformas como las redes sociales o aplicaciones de mensajería envían notificaciones cuando los usuarios realizan ciertas acciones (nuevos mensajes, comentarios, etc.).\n",
    "\n",
    "  - **Requisito de flujo:** Detectar eventos en tiempo real para desencadenar notificaciones instantáneas.\n",
    "\n",
    "2. **Informes en tiempo real:**\n",
    "\n",
    "  - **Ejemplo:** Dashboards que muestran métricas clave de rendimiento en tiempo real (como Google Analytics o herramientas de monitoreo de sistemas).\n",
    "\n",
    "  - **Requisito de flujo:** Procesar y mostrar datos a medida que son generados, sin demoras, para proporcionar visibilidad continua.\n",
    "\n",
    "3. **ETL incremental:**\n",
    "\n",
    "  - **Ejemplo:** Actualización continua de datos en un almacén de datos, donde solo se procesan y cargan los datos nuevos o modificados, optimizando recursos y tiempo.\n",
    "\n",
    "  - **Requisito de flujo:** Transformar y cargar datos de manera eficiente a medida que se reciben, en lugar de realizar cargas completas.\n",
    "\n",
    "4. **Actualización de datos para servir en tiempo real:**\n",
    "\n",
    "  - **Ejemplo:** Servicios como motores de recomendación (Netflix, Amazon) que actualizan los datos de usuarios y recomendaciones con cada nueva interacción.\n",
    "  - **Requisito de flujo:** Los modelos de datos o bases de datos deben ser actualizados con las interacciones en tiempo real para mejorar la experiencia del usuario.\n",
    "\n",
    "5. **Toma de decisiones en tiempo real:**\n",
    "\n",
    "  - **Ejemplo:** Algoritmos de detección de fraude en sistemas financieros que deben analizar transacciones y detectar comportamientos anómalos en cuestión de milisegundos.\n",
    "  - **Requisito de flujo:** Procesamiento inmediato de datos para actuar ante situaciones críticas, como bloquear transacciones sospechosas.\n",
    "\n",
    "6. **ML en línea:**\n",
    "\n",
    "  - **Ejemplo:** Aplicaciones de aprendizaje automático que se actualizan de forma continua con nuevos datos, mejorando sus predicciones de manera incremental (publicidad personalizada, precios dinámicos, etc.).\n",
    "  - **Requisito de flujo:** Procesar y alimentar los modelos con datos nuevos sin necesidad de volver a entrenar el modelo desde cero.\n",
    "\n",
    "\n",
    "Cada uno de estos casos utiliza herramientas de procesamiento de flujos como **Apache Kafka, Apache Flink, Spark Streaming o Databricks Structured Streaming**, que permiten manejar el flujo continuo de datos y tomar decisiones basadas en los eventos que ocurren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884e9278-724e-4d76-8311-9dc49504ee9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; background-color: lightblue; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Conjunto de datos delimitado frente a conjunto de datos no delimitado**\n",
    "\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "  **Datos limitados**\n",
    "\n",
    "  - Tiene una estructura finita e invariable en el momento del procesamiento\n",
    "  - El orden es estático\n",
    "  - **Analogía:** Vehículos en un aparcamiento.\n",
    "\n",
    "  </div>\n",
    "\n",
    "  <div style=\"flex: 50%; padding: 10px;\">\n",
    "\n",
    "  **Datos no limitados**\n",
    "\n",
    "  - Tiene una estructura infinita y continuamente cambiante en el momento del procesamiento.\n",
    "  - El orden no siempre es secuencial.\n",
    "  - **Analogía:** Vehículos en una autopista\n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f635834e-36d8-4b2c-8abf-d9d68dd06751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Notas**\n",
    "\n",
    "En el contexto del procesamiento de datos, los términos \"bounded\" y \"unbounded\" se refieren a los tipos de conjuntos de datos con los que se trabaja, y estos conceptos son clave en la elección de enfoques y herramientas de procesamiento de datos. A continuación, te explico ambos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b447f597-00b3-4c48-aea6-9fec474e1436",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Bounded Dataset (Conjunto de Datos Acotado):**\n",
    "\n",
    "**Definición:** Un conjunto de datos es considerado \"bounded\" si tiene un tamaño finito y limitado. Es decir, todos los datos ya están disponibles desde el principio, o al menos se sabe cuándo se completa la llegada de los datos.\n",
    "\n",
    "**Ejemplo:**\n",
    "- Una tabla de una base de datos relacional donde todos los registros ya están presentes.\n",
    "- Un archivo CSV que contiene un número finito de registros.\n",
    "- Un lote de transacciones históricas de ventas en un archivo.\n",
    "\n",
    "**Uso:** Este tipo de conjunto de datos se maneja en procesamiento por lotes (batch processing), ya que no se espera que los datos cambien después de que se inicie el procesamiento.\n",
    "\n",
    "**Herramientas comunes:** Apache Hadoop, Apache Spark (batch mode), SQL tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c63c93-9be1-4973-b58c-85d7825aa3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Unbounded Dataset (Conjunto de Datos No Acotado):**\n",
    "\n",
    "**Definición:** Un conjunto de datos es considerado \"unbounded\" si se trata de un flujo continuo de datos sin un final conocido. Los datos se generan de manera continua y teóricamente infinita, lo que requiere procesamiento en tiempo real o cercano al tiempo real.\n",
    "\n",
    "**Ejemplo:**\n",
    "- Datos generados por sensores IoT en tiempo real.\n",
    "- Transacciones financieras o clics en sitios web que se registran de manera constante.\n",
    "- Flujos de logs de servidores web o sistemas de monitoreo de red.\n",
    "\n",
    "**Uso:** Este tipo de conjunto de datos se maneja mediante procesamiento de flujos (stream processing), donde los datos se procesan a medida que llegan, en lugar de esperar a que se complete un lote.\n",
    "\n",
    "**Herramientas comunes:** Apache Kafka, Apache Flink, Spark Streaming, Databricks Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9638938-69b9-4dad-9bc1-77cd9772ffd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Diferencias clave:**\n",
    "\n",
    "| **Característica**  | **Bounded Dataset**  | **Unbounded Dataset**  |\n",
    "|---------------------|----------------------|------------------------|\n",
    "| **Tamaño**          | Finito               | Infinito (flujo continuo) |\n",
    "| **Procesamiento**    | Procesamiento por lotes (batch) | Procesamiento de flujo (streaming) |\n",
    "| **Finalización**     | Conocido, procesamiento tiene un fin | Desconocido, datos siguen llegando |\n",
    "| **Latencia**         | Típicamente alta (el procesamiento ocurre al completar el lote) | Baja, el procesamiento es inmediato |\n",
    "| **Casos de uso**     | Informes históricos, procesamiento de grandes archivos o bases de datos | Monitorización en tiempo real, toma de decisiones instantánea |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09942d65-bd49-4fcb-bdaf-f5c9b92d1c40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Ejemplos de aplicación:**\n",
    "\n",
    "**Bounded Dataset:** Cuando realizas análisis sobre todos los datos históricos de una empresa, ya que puedes cargar todo el conjunto de datos a la vez y ejecutar consultas o transformaciones.\n",
    "\n",
    "**Unbounded Dataset:** Cuando se realiza el monitoreo de eventos de una red de servidores, donde las alertas deben dispararse en tiempo real ante fallos o actividades anómalas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9815f8b-6bce-4d87-937f-6ad6866f1f89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"background-color: lightblue; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "  **Procesamiento por lotes frente a secuencias**\n",
    "\n",
    "  Procesamiento por lotes\n",
    "\n",
    "  - Generalmente se refiere al procesamiento y análisis de conjuntos de datos acotados (es decir, el tamaño es bien conocido, podemos contar el número de elementos, etc.)\n",
    "  - Típico de aplicaciones en las que la latencia de los datos es variable (es decir, de un día, una semana o un mes).\n",
    "  - Se trataba de un ETL tradicional de sistemas transaccionales a sistemas analíticos\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ebed9b7-bb68-4561-b6cb-6d1ecbfca37f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Notas**\n",
    "\n",
    "El tratamiento por lotes es uno de los enfoques más tradicionales en el procesamiento de datos. Aquí te presento cómo se estructura un flujo típico de procesamiento por lotes, partiendo de las características que mencionas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be88aedc-9c3b-4f12-b11d-1e0792e5c650",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**1. Fuentes de datos limitadas:**\n",
    "\n",
    "**Definición:** Las fuentes de datos en el procesamiento por lotes suelen estar predefinidas y son finitas. Esto significa que los datos a procesar ya están completos o son recolectados durante un período de tiempo determinado antes de su procesamiento.\n",
    "\n",
    "**Ejemplos comunes:**\n",
    "- Archivos estáticos (como CSV, JSON, o Parquet).\n",
    "- Bases de datos relacionales (MySQL, PostgreSQL, SQL Server).\n",
    "- Logs históricos almacenados.\n",
    "- Extractos de datos de APIs o sistemas externos en intervalos fijos (diarios, semanales).\n",
    "\n",
    "**Objetivo:** Recoger todos los datos de estas fuentes para procesarlos en un único lote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "438e0b77-d101-412a-a7cb-7ee80df30f46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**2. Ingesta de datos por lotes:**\n",
    "\n",
    "**Definición:** El proceso de ingesta por lotes implica la recolección de grandes cantidades de datos en bloques y cargarlos en un sistema de almacenamiento o procesamiento para ser tratados posteriormente.\n",
    "\n",
    "**Proceso:**\n",
    "- Los datos se obtienen de las fuentes a intervalos predeterminados o de manera manual.\n",
    "- Una vez recolectados, los datos se mueven hacia una ubicación donde pueden ser procesados (almacenamiento en el clúster de procesamiento o en un sistema de archivos distribuido como HDFS).\n",
    "\n",
    "**Herramientas para la ingesta:**\n",
    "- Sqoop: Para importar datos de bases de datos relacionales a sistemas Hadoop o Spark.\n",
    "- Apache NiFi: Para el flujo y transformación de datos en lotes.\n",
    "- Python, Bash scripts: Para mover archivos y automatizar la ingesta de datos desde diferentes fuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51dd8397-d99f-4d61-b70c-edd443711082",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**3. Capa de almacenamiento y motor de procesamiento por lotes:**\n",
    "\n",
    "**Definición:** Una vez que los datos son recolectados, se almacenan en una capa de almacenamiento adecuado, desde donde serán procesados en bloques o lotes por un motor de procesamiento. El objetivo es transformar o analizar estos datos de manera eficiente.\n",
    "\n",
    "**Capa de almacenamiento:**\n",
    "- HDFS (Hadoop Distributed File System): Es el sistema de almacenamiento distribuido más común en sistemas de procesamiento por lotes, utilizado para almacenar grandes volúmenes de datos que necesitan ser procesados.\n",
    "- Data Lakes: Sistemas como Amazon S3, Azure Data Lake Storage o Google Cloud Storage, utilizados para almacenar y organizar grandes conjuntos de datos en crudo o semi-estructurados.\n",
    "- Bases de datos relacionales o NoSQL: Dependiendo del volumen y la estructura de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34a62b8-191a-433d-974b-d3d79bd4bf07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Motores de procesamiento por lotes:**\n",
    "\n",
    "- Apache Hadoop (MapReduce): Utilizado para el procesamiento distribuido de grandes lotes de datos. Divide el trabajo en pequeños subtareas que se ejecutan en paralelo.\n",
    "- Apache Spark (modo batch): Una alternativa más rápida y eficiente que Hadoop, al realizar el procesamiento en memoria (aunque también soporta procesamiento en disco). Spark permite realizar transformaciones y análisis complejos de datos con menor latencia.\n",
    "- Databricks (modo batch): Plataforma basada en Spark que permite gestionar y escalar automáticamente los clústeres, simplificando el procesamiento de grandes volúmenes de datos en lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36df6947-dd9c-4ca4-a5e0-ba46ddd66854",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**4. Motor de consulta:**\n",
    "\n",
    "**Definición:** El motor de consulta es el componente encargado de interactuar con los datos procesados para extraer información útil o realizar análisis complejos. Se utiliza para generar informes, dashboards o ejecutar consultas ad-hoc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43320109-8b17-4f95-b6d5-fb98937130af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Ejemplos de motores de consulta:**\n",
    "\n",
    "- Apache Hive: Un motor de consulta SQL diseñado para trabajar sobre Hadoop. Tradicionalmente usa MapReduce, pero puede utilizar motores más rápidos como Tez o Spark.\n",
    "- Presto: Un motor SQL distribuido de alto rendimiento, ideal para ejecutar consultas rápidas sobre grandes volúmenes de datos en sistemas como HDFS, S3 o incluso bases de datos relacionales.\n",
    "- Spark SQL: Parte del ecosistema de Apache Spark que permite ejecutar consultas SQL sobre datos almacenados en DataFrames o conjuntos de datos (datasets). Al aprovechar la capacidad de procesamiento en memoria de Spark, ofrece consultas rápidas y eficientes.\n",
    "- Databricks SQL: Un motor basado en Spark SQL que permite ejecutar consultas SQL a través de Databricks. Es útil para interactuar con grandes volúmenes de datos en entornos de análisis en la nube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90cce688-3d7c-448c-967f-fa9f9e192eea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Ejemplo de flujo de procesamiento por lotes:**\n",
    "1. Ingesta: Supongamos que tienes un sistema de ventas que almacena transacciones diarias en archivos CSV. Cada día, se genera un archivo nuevo con todas las ventas del día.\n",
    "2. Almacenamiento: Estos archivos se cargan diariamente a un sistema de almacenamiento como HDFS o un Data Lake (por ejemplo, S3).\n",
    "3. Procesamiento: Cada semana, se ejecuta un trabajo por lotes que lee todos los archivos CSV almacenados, los procesa (limpieza, agregación, transformación) y los almacena en un formato optimizado como Parquet.\n",
    "4. Consulta: Posteriormente, un motor de consulta como Hive o Spark SQL ejecuta consultas sobre estos datos procesados para generar informes o dashboards de análisis de ventas semanales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fd3d83-92a1-474e-9079-a326375bcff5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**Ejemplo de código para procesamiento por lotes en PySpark:**\n",
    "\n",
    "---\n",
    "\n",
    "Ejemplo sencillo de cómo podría verse el procesamiento por lotes en PySpark utilizando archivos CSV de ventas, almacenándolos como Parquet, y luego ejecutando una consulta sobre los datos procesados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750f6ead-1504-468b-aee9-77d20f7c8f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"Batch Processing Example\").getOrCreate()\n",
    "\n",
    "# 1. Ingesta de datos (lectura de archivos CSV de ventas)\n",
    "sales_data = spark.read.csv(\"/path/to/daily_sales/*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Procesamiento por lotes (transformaciones y agregaciones)\n",
    "# Ejemplo: Calcular las ventas totales por categoría de producto\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "sales_by_category = sales_data.groupBy(\"product_category\").agg(spark_sum(\"sales_amount\").alias(\"total_sales\"))\n",
    "\n",
    "# 3. Almacenamiento (escribir los resultados procesados en formato Parquet)\n",
    "sales_by_category.write.mode(\"overwrite\").parquet(\"/path/to/output/sales_by_category.parquet\")\n",
    "\n",
    "# 4. Motor de consulta (leer el archivo Parquet y ejecutar consultas SQL)\n",
    "processed_data = spark.read.parquet(\"/path/to/output/sales_by_category.parquet\")\n",
    "processed_data.createOrReplaceTempView(\"sales_data\")\n",
    "query_result = spark.sql(\"SELECT * FROM sales_data WHERE total_sales > 1000\")\n",
    "\n",
    "# Mostrar el resultado de la consulta\n",
    "query_result.show()\n",
    "\n",
    "# Cerrar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da78c605-02d1-4899-a26f-54fa5cc2a821",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Este flujo es ideal para situaciones donde el volumen de datos es grande pero no hay necesidad de procesarlos en tiempo real. El procesamiento por lotes sigue siendo fundamental en muchos casos donde se prioriza la eficiencia y el análisis de grandes volúmenes de datos históricos."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Incremental Processing with Structured Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
